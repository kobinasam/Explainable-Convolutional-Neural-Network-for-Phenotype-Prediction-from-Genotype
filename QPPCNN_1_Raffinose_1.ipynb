{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "#!pip install keras-tuner\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.layers import Layer\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "from matplotlib import pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow.python.ops.numpy_ops import np_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "genotypeFile = 'genotype.csv'\n",
    "genotype = pd.read_csv(genotypeFile, sep = '\\t', index_col = 0)\n",
    "print('genotypeFile shape:', genotype.shape )\n",
    "\n",
    "phenotypeFile = 'phenotype.csv'\n",
    "multi_pheno = pd.read_csv(phenotypeFile, sep = ',', index_col = 0)\n",
    "print('Phenotype_Multi shape:', multi_pheno.shape )\n",
    "\n",
    "\n",
    "# take a small part to test code\n",
    "# genotype\n",
    "X = genotype\n",
    "#display(X.head())\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "\n",
    "#Chage the index for each model here accoding to the correspinding model\n",
    "Y = multi_pheno.iloc[:, 14]#index=1 --> 1_Raffinose_1\n",
    "# Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "# corrupted_X.head()\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "# corrupted_X_onehot.shape\n",
    "\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430eae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_list = list([i for i in range(404)])[0:100]\n",
    "if len(background_list)==0:\n",
    "    background_list = x_train[np.random.choice(x_train.shape[0],100, replace=False)]\n",
    "background = x_train[background_list]\n",
    "background.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from shap import initjs\n",
    "initjs()\n",
    "custom_objects = None\n",
    "# Define custom objects if necessary\n",
    "custom_objects = None\n",
    "\n",
    "# Load the model with specified reduction strategy\n",
    "model_mse = tf.keras.models.load_model('1_Raffinose_1.h5', custom_objects=custom_objects, compile=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44320628-e5ae-4717-9876-75eca5078202",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model_mse,x_test)\n",
    "base_value = explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e42ca-1235-4bcb-bec0-1184dfbb2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_values = base_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219316d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(x_test,check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0b64f-5e24-490f-8a10-d2fc6a385d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shap_values = np.mean(shap_values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0abd94-2e3f-49fb-ad9c-1632262620f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_shap_values = np.abs(shap_values[0])\n",
    "feature_importance = np.mean(abs_shap_values, axis=0)\n",
    "len(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0017c9f-897b-4daa-9f84-df6a8c2c3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42dfc0e-532f-490d-9478-cbd585e5217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_impportance_sum = feature_importance.sum(axis=1)\n",
    "feature_impportance_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca145b-7491-4171-a690-7a5e217cdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns.tolist(),  # Convert X.columns to a list\n",
    "    'Importance': feature_impportance_sum.tolist()\n",
    "})\n",
    "\n",
    "#feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31125c14-97c8-4241-b9de-115d2daae30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = feature_importance_df[feature_importance_df['Importance']  <= 1e-4]\n",
    "less_important_features = num.Feature.tolist()\n",
    "less_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21556116-422e-43c8-891d-0658e8cc8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = feature_importance_df[feature_importance_df['Importance']  >= 1e-4]\n",
    "important_features = num.Feature.tolist()\n",
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a093ff3-29b9-44ed-84e3-043e00a9e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(main_list, search_list):\n",
    "    return [i for i, x in enumerate(main_list) if x in search_list]\n",
    "\n",
    "indexes_shap = find_indexes(feature_importance_df['Feature'].tolist(), important_features)\n",
    "print(len(indexes_shap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165763af-f52f-4e56-9cff-dbb0a3d05251",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_dict = dict(zip(feature_importance_df['Feature'], feature_importance_df['Importance']))\n",
    "feature_importance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff8924-5331-43fd-a508-613700b86495",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#### First we want to remove all values that are less than and equal to zero and save them in a list\n",
    "\n",
    "removed_pairs_features = []\n",
    "\n",
    "filtered_pairs_features = [(key, value) for key, value in sorted_features if value > 0]\n",
    "removed_pairs_features = [(key, value) for key, value in sorted_features if value <= 0]\n",
    "filtered_pairs_features_numpy = np.array(filtered_pairs_features)\n",
    "# print(\"Filtered key-value pairs:\", filtered_pairs_features)\n",
    "# print(\"Removed key-value pairs:\", removed_pairs_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66452a48-1405-4832-8052-e59f6bdc5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature names and importance values for visualization\n",
    "sorted_feature_names = [x[0] for x in filtered_pairs_features[:20]]\n",
    "sorted_feature_importance = [x[1] for x in filtered_pairs_features[:20]]\n",
    "\n",
    "# Plot the ranked features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_feature_names, sorted_feature_importance)\n",
    "plt.xlabel('SHAP Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Ranked Features based on SHAP Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50366b-8da6-443e-b0e1-3b3761ce9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_shap_values = [pair[1] for pair in filtered_pairs_features[:30]]\n",
    "feature_shap_values_names = [pair[0] for pair in filtered_pairs_features[:30]]\n",
    "feature_names = np.array(feature_shap_values_names)\n",
    "type(feature_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9bc59-7641-478a-af7e-79b754d6dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a list of features and their corresponding SHAP values\n",
    "features = feature_importance_df['Feature'][:20]\n",
    "shap_values = feature_importance_df['Importance'][:20]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(shap_values, features, color='green', s=100, alpha=0.6)\n",
    "plt.xlabel('SHAP Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature with their corresponding SHAP value')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7dce96-e7af-42e5-a55a-faf0ac73811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating the FORCE Plot\n",
    "shap_values = np.array(feature_shap_values)  # Assuming 100 samples and 5 features\n",
    "shap_values_ = np.reshape(shap_values, (-1,2))\n",
    "shap.force_plot(base_values, shap_values, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e6bf1-d406-41a7-8c94-c29e4a5d1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating the Waterfall Plot\n",
    "\n",
    "explanation = shap.Explanation(values=shap_values , base_values=base_values, feature_names = feature_names)\n",
    "\n",
    "shap.waterfall_plot(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd76c2-0857-4a28-8b2f-e47c4802bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ba848-05b4-4285-8a0f-fb130d247c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset with a categorical column\n",
    "data = genotype\n",
    "df_data = pd.DataFrame(data)\n",
    "# Perform one-hot encoding on a specific categorical column\n",
    "encoded_df = pd.get_dummies(df_data)\n",
    "\n",
    "# Display the dataset after one-hot encoding\n",
    "print(\"\\nDataset after One-Hot Encoding:\")\n",
    "print(encoded_df)\n",
    "encoded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b79c10-3fdd-477e-873a-8243c47b41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample one-hot encoded dataset\n",
    "data = encoded_df\n",
    "df_data = pd.DataFrame(data)\n",
    "\n",
    "# Select features to set one-hot encoding to zero\n",
    "selected_features = less_important_features\n",
    "\n",
    "# Set one-hot encoding of selected features to zero\n",
    "for feature in selected_features:\n",
    "    for col in df_data.columns:\n",
    "        if feature in col:\n",
    "            df_data[col] = 0\n",
    "\n",
    "# Check the modified dataset\n",
    "print(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c54342-fca7-4e8a-89a7-7b9964c484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data['5493023_chrIX_100450_T_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274f387-0f23-4cf0-ba48-29e5d321b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "Y = multi_pheno.iloc[:, 14]#index=1 --> 1_Raffinose_1\n",
    "# Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "print(corrupted_X['1796048_chrIV_436026_T_C'])\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "print(type(corrupted_X_onehot))\n",
    "#print(corrupted_X['6810072_chrXI_231860_A_G'])\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40182331-2326-49fd-a196-dae4d10a7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Retrain the model on the new dataset**\n",
    "\n",
    "def model_build(pheno_name,num_hl,hl_list,hl_activation, out_activation, dropout_val,filters_, kernel_size_,stride_poolSize,Conv_Layers_Stride_Size):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    https://towardsdatascience.com/multi-output-model-with-tensorflow-keras-functional-api-875dd89aa7c6\n",
    "\n",
    "    def DNN_build(num_hl,hl_list,hl_activation, out_activation, dropout_val):\n",
    "    pheno_name = name of phenotype of interest\n",
    "    num_hl = number of hidden layers\n",
    "    hl_list = list of hidden layers\n",
    "    hl_activation = hidden layer activation function\n",
    "    out_activation = output layer activation function\n",
    "    dropout_val = Dropout value\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    input_layer = tf.keras.Input(shape=(28220,3))#(3138, 28220, 3)\n",
    "    \n",
    "    shared_convL1 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation,input_shape = (28220,3))(input_layer)\n",
    "    shared_convL1_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL1)\n",
    "    \n",
    "    shared_convL2 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation)(shared_convL1_max_pool)\n",
    "    shared_convL2_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL2)\n",
    "    \n",
    "    shared_convLayer_Flatten = tf.keras.layers.Flatten()(shared_convL2_max_pool)\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    kernel_regularizer_ = tf.keras.regularizers.L1L2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    shared_hidden_layer1 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[0], activation  = hl_activation)(shared_convLayer_Flatten)\n",
    "    shared_hidden_layer1_dp1 = tf.keras.layers.Dropout(dropout_val[0])(shared_hidden_layer1)\n",
    "\n",
    "    #model 1\n",
    "    model1_hidden_layer2 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[1], activation  = hl_activation)(shared_hidden_layer1_dp1)\n",
    "    model1_hidden_layer2_dp2 = tf.keras.layers.Dropout(dropout_val[1])(model1_hidden_layer2)\n",
    "\n",
    "    model1_hidden_layer3 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[2], activation  = hl_activation)(model1_hidden_layer2_dp2)\n",
    "    model1_hidden_layer3_dp3 = tf.keras.layers.Dropout(dropout_val[2])(model1_hidden_layer3)\n",
    "\n",
    "    model1_hidden_layer4 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[3], activation  = hl_activation)(model1_hidden_layer3_dp3)\n",
    "    model1_hidden_layer4_dp4 = tf.keras.layers.Dropout(dropout_val[3])(model1_hidden_layer4)\n",
    "    model1_hidden_layer4_dp4_fl = tf.keras.layers.Flatten()(model1_hidden_layer4_dp4)\n",
    "    \n",
    "    pheno_name_ = tf.keras.layers.Dense(units = 1, name = pheno_name, activation  = out_activation)(model1_hidden_layer4_dp4_fl)#1_CobaltChloride_1\n",
    "    model = tf.keras.models.Model(inputs = input_layer, outputs = [pheno_name_])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a75a83-30d1-45e6-9eb8-f402ce084662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy','mse'])#change metrics to MSE\n",
    "def compile_model(pheno_name,model,loss_, learningRate, metrics_): #loss = 'sparse_categorical_crossentropy'\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest ['accuracy', 'mse']\n",
    "    '''\n",
    "\n",
    "    # Specify the optimizer, and compile the model with loss functions for both outputs\n",
    "    model.compile(\n",
    "       #optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate),#0.00027705 \n",
    "    optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learningRate, momentum=0.01, nesterov=True, name=\"SGD\"),\n",
    "      loss = {\n",
    "          pheno_name:loss_\n",
    "      }, \n",
    "      metrics = [metrics_]#{'CobaltChloride_1':[metrics_],'CopperSulfate_1':[metrics_],'Diamide_1':[metrics_]}\n",
    "      )#change metrics to MSE ##['accuracy','mse']\n",
    "    return model\n",
    "\n",
    "def runModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode,pheno_index):\n",
    "    '''\n",
    "    def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "    pheno_index: the index of the phenotype in y_train\n",
    "    '''\n",
    "    history = model.fit(\n",
    "    x_train, \n",
    "    [y_train[:]],\n",
    "    validation_data=(x_valid, [y_valid[:]]),\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d6fc1-4799-4e89-b064-cb632db23543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel_all(pheno_name, pheno_index,hl_list, drop_list):\n",
    "    model_mse = model_build(pheno_name,4,hl_list,'relu','linear',drop_list,8,7,2,3)#model initiallization\n",
    "    model_mse = compile_model(pheno_name,model_mse,tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mse\"),\n",
    "    0.00123695244,'mse')\n",
    "    history_mse =  runModel(model_mse,0.30,32,400,15, \"val_mse\", \"min\",pheno_index)\n",
    "    return history_mse,model_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81271118-5a5c-4a1b-9de3-3710b251ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mse,model_mse = runModel_all('1_Raffinose_1',14,[1464,608,264,146],[0.00,0.00,0.00,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374ad1d-c905-4dfe-ab81-2b163df853b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.save('important features/saved Models/1_Raffinose_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d2c35-aeae-4e68-96c3-d66fb5c59743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model_mse.predict(x_valid)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "print(\"Mean Squared Error on the validation set:\", mse)\n",
    "\n",
    "# Make predictions on the tesing set\n",
    "y_pred = model_mse.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on the testing set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd899e2b-78b1-466f-bbe6-2e12e81ee0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is for LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "model_mse= tf.keras.models.load_model('1_Raffinose_1.h5')\n",
    "print(type(genotype))\n",
    "genotype_data = genotype.to_numpy()\n",
    "class_names = ['Positive','Negative']\n",
    "my_features = genotype.columns.tolist()\n",
    "#model_mse_ = tf.keras.models.load_model('important features/Models/1_CobaltChloride_1.h5', custom_objects=custom_objects, compile=False)\n",
    "\n",
    "explainer = lime.lime_tabular.RecurrentTabularExplainer(x_train, mode='regression', training_labels=None, feature_names=my_features, \n",
    "                        categorical_features=None, categorical_names=None, kernel_width=None, kernel=None, verbose=False, \n",
    "                        class_names=class_names, feature_selection='auto', discretize_continuous=True, discretizer='quartile', \n",
    "                         random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b495579-c47e-41ba-ae0f-75d6ed407814",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "instance = x_test[i]\n",
    "exp = explainer.explain_instance(instance, model_mse.predict, num_features = len(my_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33d5ed-e168-4e44-aae5-302321cf44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()\n",
    "lime_importance = dict(exp.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2170f8-7ebb-472f-9250-92ccb49d232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_lime_importance = dict(sorted(lime_importance.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_lime_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a6f48-7082-47ac-99b4-32d5be860e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_important_lime = [k for k, v in sorted_lime_importance.items() if v <= 1e-5]\n",
    "len(less_important_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590eef82-a420-4a9d-b78e-e8549698e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_lime = [k for k, v in lime_importance.items() if v >= 1e-5]\n",
    "len(important_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416aa9f-9013-4dc6-a0f2-5e233dc1752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(main_list, search_list):\n",
    "    return [i for i, x in enumerate(main_list) if x in search_list]\n",
    "\n",
    "indexes_lime = find_indexes(lime_importance, important_lime)\n",
    "len(indexes_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a480fb7-60e7-4d1f-8652-73a3a0d3c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(main_list, search_list):\n",
    "    return [i for i, x in enumerate(main_list) if x in search_list]\n",
    "\n",
    "indexes = find_indexes(lime_importance, less_important_lime)\n",
    "print(indexes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664053bc-09dc-487b-8b5e-33e4501c5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_lime_importance_list = sorted(sorted_lime_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "num_features_to_plot = min(20, len(sorted_lime_importance_list))\n",
    "\n",
    "# Extract feature names and importance values for visualization\n",
    "sorted_lime_names = [x[0] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "sorted_lime_importance_ = [x[1] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "\n",
    "# Plot the ranked features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_lime_names, sorted_lime_importance_)\n",
    "plt.xlabel('LIME Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Ranked Features based on LIME Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6d084-052f-4a7d-b015-526a8b1c0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_lime_importance_list = sorted(sorted_lime_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "num_features_to_plot = min(20, len(sorted_lime_importance_list))\n",
    "\n",
    "# Extract feature names and importance values for visualization\n",
    "sorted_lime_names = [x[0] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "sorted_lime_importance_ = [x[1] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(sorted_lime_importance_, sorted_lime_names, color='green', s=100, alpha=0.6)\n",
    "plt.xlabel('LIME Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature with their corresponding SHAP value')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc15682-f4e7-4a6b-b939-712a47c8a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset with a categorical column\n",
    "data_lime = genotype\n",
    "df_data = pd.DataFrame(data_lime)\n",
    "# Perform one-hot encoding on a specific categorical column\n",
    "encoded_df = pd.get_dummies(df_data)\n",
    "\n",
    "# Display the dataset after one-hot encoding\n",
    "print(\"\\nDataset after One-Hot Encoding:\")\n",
    "print(encoded_df)\n",
    "encoded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced70358-c314-433a-a2db-0ed145abc71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample one-hot encoded dataset\n",
    "data = encoded_df\n",
    "df_data_ = pd.DataFrame(data)\n",
    "type(df_data.columns)\n",
    "# Select features to set one-hot encoding to zero\n",
    "selected_features = indexes\n",
    "\n",
    "# Set one-hot encoding of selected features to zero\n",
    "for index in selected_features:\n",
    "    for col in df_data_.columns:\n",
    "        if index in df_data_.columns:\n",
    "            df_data_[col] = 0\n",
    "\n",
    "# Check the modified dataset\n",
    "print(df_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798676f8-1bfe-430a-95ba-e739313f2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data_\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "Y = multi_pheno.iloc[:, 14]#index=1 --> 1_Raffinose_1\n",
    "# Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "print(type(corrupted_X_onehot))\n",
    "#print(corrupted_X['6810072_chrXI_231860_A_G'])\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ad441-ee57-4b07-9408-d1931d6503c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Retrain the model on the new dataset**\n",
    "\n",
    "def model_build(pheno_name,num_hl,hl_list,hl_activation, out_activation, dropout_val,filters_, kernel_size_,stride_poolSize,Conv_Layers_Stride_Size):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    https://towardsdatascience.com/multi-output-model-with-tensorflow-keras-functional-api-875dd89aa7c6\n",
    "\n",
    "    def DNN_build(num_hl,hl_list,hl_activation, out_activation, dropout_val):\n",
    "    pheno_name = name of phenotype of interest\n",
    "    num_hl = number of hidden layers\n",
    "    hl_list = list of hidden layers\n",
    "    hl_activation = hidden layer activation function\n",
    "    out_activation = output layer activation function\n",
    "    dropout_val = Dropout value\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    input_layer = tf.keras.Input(shape=(28220,3))#(3138, 28220, 3)\n",
    "    \n",
    "    shared_convL1 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation,input_shape = (28220,3))(input_layer)\n",
    "    shared_convL1_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL1)\n",
    "    \n",
    "    shared_convL2 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation)(shared_convL1_max_pool)\n",
    "    shared_convL2_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL2)\n",
    "    \n",
    "    shared_convLayer_Flatten = tf.keras.layers.Flatten()(shared_convL2_max_pool)\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    kernel_regularizer_ = tf.keras.regularizers.L1L2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    shared_hidden_layer1 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[0], activation  = hl_activation)(shared_convLayer_Flatten)\n",
    "    shared_hidden_layer1_dp1 = tf.keras.layers.Dropout(dropout_val[0])(shared_hidden_layer1)\n",
    "\n",
    "    #model 1\n",
    "    model1_hidden_layer2 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[1], activation  = hl_activation)(shared_hidden_layer1_dp1)\n",
    "    model1_hidden_layer2_dp2 = tf.keras.layers.Dropout(dropout_val[1])(model1_hidden_layer2)\n",
    "\n",
    "    model1_hidden_layer3 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[2], activation  = hl_activation)(model1_hidden_layer2_dp2)\n",
    "    model1_hidden_layer3_dp3 = tf.keras.layers.Dropout(dropout_val[2])(model1_hidden_layer3)\n",
    "\n",
    "    model1_hidden_layer4 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[3], activation  = hl_activation)(model1_hidden_layer3_dp3)\n",
    "    model1_hidden_layer4_dp4 = tf.keras.layers.Dropout(dropout_val[3])(model1_hidden_layer4)\n",
    "    model1_hidden_layer4_dp4_fl = tf.keras.layers.Flatten()(model1_hidden_layer4_dp4)\n",
    "    \n",
    "    pheno_name_ = tf.keras.layers.Dense(units = 1, name = pheno_name, activation  = out_activation)(model1_hidden_layer4_dp4_fl)#1_CobaltChloride_1\n",
    "    model = tf.keras.models.Model(inputs = input_layer, outputs = [pheno_name_])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0c401-6f75-4870-8802-92add8f9ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy','mse'])#change metrics to MSE\n",
    "def compile_model(pheno_name,model,loss_, learningRate, metrics_): #loss = 'sparse_categorical_crossentropy'\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest ['accuracy', 'mse']\n",
    "    '''\n",
    "\n",
    "    # Specify the optimizer, and compile the model with loss functions for both outputs\n",
    "    model.compile(\n",
    "       #optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate),#0.00027705 \n",
    "    optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learningRate, momentum=0.01, nesterov=True, name=\"SGD\"),\n",
    "      loss = {\n",
    "          pheno_name:loss_\n",
    "      }, \n",
    "      metrics = [metrics_]#{'CobaltChloride_1':[metrics_],'CopperSulfate_1':[metrics_],'Diamide_1':[metrics_]}\n",
    "      )#change metrics to MSE ##['accuracy','mse']\n",
    "    return model\n",
    "\n",
    "def runModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode,pheno_index):\n",
    "    '''\n",
    "    def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "    pheno_index: the index of the phenotype in y_train\n",
    "    '''\n",
    "    history = model.fit(\n",
    "    x_train, \n",
    "    [y_train[:]],\n",
    "    validation_data=(x_valid, [y_valid[:]]),\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bb02c-4103-4a4c-a15d-6b54982b46af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel_all(pheno_name, pheno_index,hl_list, drop_list):\n",
    "    model_mse = model_build(pheno_name,4,hl_list,'relu','linear',drop_list,8,7,2,3)#model initiallization\n",
    "    model_mse = compile_model(pheno_name,model_mse,tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mse\"),\n",
    "    0.00123695244,'mse')\n",
    "    history_mse =  runModel(model_mse,0.30,32,400,15, \"val_mse\", \"min\",pheno_index)\n",
    "    return history_mse,model_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1bc8d-6c19-44ca-a685-9f59acf26e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mse,model_mse = runModel_all('1_Raffinose_1',14,[1464,608,264,146],[0.00,0.00,0.00,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758b28e-927c-48ed-8cb8-fc3e517e0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model_mse.predict(x_valid)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "print(\"Mean Squared Error on the validation set:\", mse)\n",
    "\n",
    "# Make predictions on the tesing set\n",
    "y_pred = model_mse.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on the testing set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de695fa3-c976-45dc-9bcb-c76f32eb54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Intersection of LIME and SHAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76251caf-b655-4da4-878a-c943f1cf599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_indexes(lime_indexes, shap_indexes):\n",
    "    # Convert lists to sets\n",
    "    lime_set = set(lime_indexes)\n",
    "    shap_set = set(shap_indexes)\n",
    "    \n",
    "    # Find the intersection\n",
    "    common_indexes = lime_set.intersection(shap_set)\n",
    "    \n",
    "    return common_indexes\n",
    "    \n",
    "\n",
    "common_indexes = find_common_indexes(indexes_lime, indexes_shap)\n",
    "print(type(common_indexes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f4edf-0548-4dba-9e1d-51654f32c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indexes = np.array(list(common_indexes))\n",
    "(common_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e53bc-1bf6-4102-8314-b40dfe113155",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indexes_ = common_indexes.tolist()\n",
    "len(common_indexes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1344b94-771d-4993-8052-106c10ea1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strings_at_indices(strings, indices):\n",
    "\n",
    "    valid_indices = [index for index in indices if 0 <= index < len(strings)]\n",
    "    \n",
    "    # Get strings at valid indices\n",
    "    result = [strings[index] for index in valid_indices]\n",
    "    \n",
    "    return result\n",
    "\n",
    "matching_features = get_strings_at_indices(feature_importance_df['Feature'].tolist(), common_indexes_)\n",
    "len(matching_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e022c9-ad0e-49b2-8c02-cfb6c925e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "genotypeFile = 'genotype.csv'\n",
    "genotype = pd.read_csv(genotypeFile, sep = '\\t', index_col = 0)\n",
    "print('genotypeFile shape:', genotype.shape )\n",
    "\n",
    "phenotypeFile = 'phenotype.csv'\n",
    "multi_pheno = pd.read_csv(phenotypeFile, sep = ',', index_col = 0)\n",
    "print('Phenotype_Multi shape:', multi_pheno.shape )\n",
    "\n",
    "\n",
    "# take a small part to test code\n",
    "# genotype\n",
    "X = genotype\n",
    "\n",
    "mask = ~X.columns.str.contains('|'.join(matching_features))\n",
    "\n",
    "# Set values to zero for non-matching columns\n",
    "X.loc[:, mask] = 0\n",
    "\n",
    "#display(X.head())\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "\n",
    "#Chage the index for each model here accoding to the correspinding model\n",
    "Y = multi_pheno.iloc[:, 14]#index=0 --> 1_Raffinose_1\n",
    "# # Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "# corrupted_X.head()\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "# corrupted_X_onehot.shape\n",
    "\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736bdf9-37d5-42b6-94be-4fbef1047d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Retrain the model on the new dataset**\n",
    "\n",
    "def model_build(pheno_name,num_hl,hl_list,hl_activation, out_activation, dropout_val,filters_, kernel_size_,stride_poolSize,Conv_Layers_Stride_Size):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    https://towardsdatascience.com/multi-output-model-with-tensorflow-keras-functional-api-875dd89aa7c6\n",
    "\n",
    "    def DNN_build(num_hl,hl_list,hl_activation, out_activation, dropout_val):\n",
    "    pheno_name = name of phenotype of interest\n",
    "    num_hl = number of hidden layers\n",
    "    hl_list = list of hidden layers\n",
    "    hl_activation = hidden layer activation function\n",
    "    out_activation = output layer activation function\n",
    "    dropout_val = Dropout value\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    input_layer = tf.keras.Input(shape=(28220,3))#(3138, 28220, 3)\n",
    "    \n",
    "    shared_convL1 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation,input_shape = (28220,3))(input_layer)\n",
    "    shared_convL1_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL1)\n",
    "    \n",
    "    shared_convL2 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation)(shared_convL1_max_pool)\n",
    "    shared_convL2_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL2)\n",
    "    \n",
    "    shared_convLayer_Flatten = tf.keras.layers.Flatten()(shared_convL2_max_pool)\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    kernel_regularizer_ = tf.keras.regularizers.L1L2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    shared_hidden_layer1 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[0], activation  = hl_activation)(shared_convLayer_Flatten)\n",
    "    shared_hidden_layer1_dp1 = tf.keras.layers.Dropout(dropout_val[0])(shared_hidden_layer1)\n",
    "\n",
    "    #model 1\n",
    "    model1_hidden_layer2 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[1], activation  = hl_activation)(shared_hidden_layer1_dp1)\n",
    "    model1_hidden_layer2_dp2 = tf.keras.layers.Dropout(dropout_val[1])(model1_hidden_layer2)\n",
    "\n",
    "    model1_hidden_layer3 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[2], activation  = hl_activation)(model1_hidden_layer2_dp2)\n",
    "    model1_hidden_layer3_dp3 = tf.keras.layers.Dropout(dropout_val[2])(model1_hidden_layer3)\n",
    "\n",
    "    model1_hidden_layer4 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[3], activation  = hl_activation)(model1_hidden_layer3_dp3)\n",
    "    model1_hidden_layer4_dp4 = tf.keras.layers.Dropout(dropout_val[3])(model1_hidden_layer4)\n",
    "    model1_hidden_layer4_dp4_fl = tf.keras.layers.Flatten()(model1_hidden_layer4_dp4)\n",
    "    \n",
    "    pheno_name_ = tf.keras.layers.Dense(units = 1, name = pheno_name, activation  = out_activation)(model1_hidden_layer4_dp4_fl)#1_CobaltChloride_1\n",
    "    model = tf.keras.models.Model(inputs = input_layer, outputs = [pheno_name_])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53a92e-58b7-4266-ad22-1dc1e225e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy','mse'])#change metrics to MSE\n",
    "def compile_model(pheno_name,model,loss_, learningRate, metrics_): #loss = 'sparse_categorical_crossentropy'\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest ['accuracy', 'mse']\n",
    "    '''\n",
    "\n",
    "    # Specify the optimizer, and compile the model with loss functions for both outputs\n",
    "    model.compile(\n",
    "       #optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate),#0.00027705 \n",
    "    optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learningRate, momentum=0.01, nesterov=True, name=\"SGD\"),\n",
    "      loss = {\n",
    "          pheno_name:loss_\n",
    "      }, \n",
    "      metrics = [metrics_]#{'CobaltChloride_1':[metrics_],'CopperSulfate_1':[metrics_],'Diamide_1':[metrics_]}\n",
    "      )#change metrics to MSE ##['accuracy','mse']\n",
    "    return model\n",
    "\n",
    "def runModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode,pheno_index):\n",
    "    '''\n",
    "    def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "    pheno_index: the index of the phenotype in y_train\n",
    "    '''\n",
    "    history = model.fit(\n",
    "    x_train, \n",
    "    [y_train[:]],\n",
    "    validation_data=(x_valid, [y_valid[:]]),\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6da197-04a1-4fb0-8fca-41c5a68ff738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel_all(pheno_name, pheno_index,hl_list, drop_list):\n",
    "    model_mse = model_build(pheno_name,4,hl_list,'relu','linear',drop_list,8,7,2,3)#model initiallization\n",
    "    model_mse = compile_model(pheno_name,model_mse,tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mse\"),\n",
    "    0.00123695244,'mse')\n",
    "    history_mse =  runModel(model_mse,0.30,32,400,15, \"val_mse\", \"min\",pheno_index)\n",
    "    return history_mse,model_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09421af0-b301-445b-9110-e8244cd590b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mse,model_mse = runModel_all('1_Raffinose_1',14,[1464,608,264,146],[0.00,0.00,0.00,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d857e9-d8f8-4740-8ced-e2c3edb96710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model_mse.predict(x_valid)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "print(\"Mean Squared Error on the validation set:\", mse)\n",
    "\n",
    "# Make predictions on the tesing set\n",
    "y_pred = model_mse.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on the testing set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b48877-1eeb-44aa-ae98-a710e156a062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
