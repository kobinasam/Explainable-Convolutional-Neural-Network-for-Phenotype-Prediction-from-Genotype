{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "#!pip install keras-tuner\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.layers import Layer\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "from matplotlib import pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow.python.ops.numpy_ops import np_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "genotypeFile = 'genotype.csv'\n",
    "genotype = pd.read_csv(genotypeFile, sep = '\\t', index_col = 0)\n",
    "print('genotypeFile shape:', genotype.shape )\n",
    "\n",
    "phenotypeFile = 'phenotype.csv'\n",
    "multi_pheno = pd.read_csv(phenotypeFile, sep = ',', index_col = 0)\n",
    "print('Phenotype_Multi shape:', multi_pheno.shape )\n",
    "\n",
    "\n",
    "# take a small part to test code\n",
    "# genotype\n",
    "X = genotype\n",
    "#display(X.head())\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "\n",
    "#Chage the index for each model here accoding to the correspinding model\n",
    "Y = multi_pheno.iloc[:, 17]#index=1 --> 1_YNB_1\n",
    "# Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "# corrupted_X.head()\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "# corrupted_X_onehot.shape\n",
    "\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430eae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_list = list([i for i in range(404)])[0:100]\n",
    "if len(background_list)==0:\n",
    "    background_list = x_train[np.random.choice(x_train.shape[0],100, replace=False)]\n",
    "background = x_train[background_list]\n",
    "background.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from shap import initjs\n",
    "initjs()\n",
    "custom_objects = None\n",
    "# Define custom objects if necessary\n",
    "custom_objects = None\n",
    "\n",
    "# Load the model with specified reduction strategy\n",
    "model_mse = tf.keras.models.load_model('1_YNB_1.h5', custom_objects=custom_objects, compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44320628-e5ae-4717-9876-75eca5078202",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model_mse,x_test)\n",
    "base_value = explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efead4cb-30d5-4b76-a653-a750e849ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_values = base_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219316d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(x_test,check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0b64f-5e24-490f-8a10-d2fc6a385d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shap_values = np.mean(shap_values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0abd94-2e3f-49fb-ad9c-1632262620f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_shap_values = np.abs(shap_values[0])\n",
    "feature_importance = np.mean(abs_shap_values, axis=0)\n",
    "len(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0017c9f-897b-4daa-9f84-df6a8c2c3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42dfc0e-532f-490d-9478-cbd585e5217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_impportance_sum = feature_importance.sum(axis=1)\n",
    "feature_impportance_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca145b-7491-4171-a690-7a5e217cdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns.tolist(),  # Convert X.columns to a list\n",
    "    'Importance': feature_impportance_sum.tolist()\n",
    "})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31125c14-97c8-4241-b9de-115d2daae30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = feature_importance_df[feature_importance_df['Importance']  <= 1e-4]\n",
    "less_important_features = num.Feature.tolist()\n",
    "less_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c889d5-527f-4895-8b2d-a585726aa732",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = feature_importance_df[feature_importance_df['Importance']  >= 1e-4]\n",
    "important_features = num.Feature.tolist()\n",
    "len(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845577e6-9641-4242-b741-e06b48360969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(main_list, search_list):\n",
    "    return [i for i, x in enumerate(main_list) if x in search_list]\n",
    "\n",
    "indexes_shap = find_indexes(feature_importance_df['Feature'].tolist(), important_features)\n",
    "print(len(indexes_shap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165763af-f52f-4e56-9cff-dbb0a3d05251",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_dict = dict(zip(feature_importance_df['Feature'], feature_importance_df['Importance']))\n",
    "feature_importance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff8924-5331-43fd-a508-613700b86495",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#### First we want to remove all values that are less than and equal to zero and save them in a list\n",
    "\n",
    "removed_pairs_features = []\n",
    "\n",
    "filtered_pairs_features = [(key, value) for key, value in sorted_features if value > 0]\n",
    "removed_pairs_features = [(key, value) for key, value in sorted_features if value <= 0]\n",
    "filtered_pairs_features_numpy = np.array(filtered_pairs_features)\n",
    "# print(\"Filtered key-value pairs:\", filtered_pairs_features)\n",
    "# print(\"Removed key-value pairs:\", removed_pairs_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66452a48-1405-4832-8052-e59f6bdc5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature names and importance values for visualization\n",
    "sorted_feature_names = [x[0] for x in filtered_pairs_features[:20]]\n",
    "sorted_feature_importance = [x[1] for x in filtered_pairs_features[:20]]\n",
    "\n",
    "# Plot the ranked features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_feature_names, sorted_feature_importance)\n",
    "plt.xlabel('SHAP Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Ranked Features based on SHAP Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50366b-8da6-443e-b0e1-3b3761ce9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_shap_values = [pair[1] for pair in filtered_pairs_features[:30]]\n",
    "feature_shap_values_names = [pair[0] for pair in filtered_pairs_features[:30]]\n",
    "feature_names = np.array(feature_shap_values_names)\n",
    "type(feature_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7613a-9e97-45ab-ad29-96a7a7370c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a list of features and their corresponding SHAP values\n",
    "features = feature_importance_df['Feature'][:20]\n",
    "shap_values = feature_importance_df['Importance'][:20]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(shap_values, features, color='green', s=100, alpha=0.6)\n",
    "plt.xlabel('SHAP Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature with their corresponding SHAP value')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7dce96-e7af-42e5-a55a-faf0ac73811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating the FORCE Plot\n",
    "shap_values = np.array(feature_shap_values)  # Assuming 100 samples and 5 features\n",
    "shap_values_ = np.reshape(shap_values, (-1,2))\n",
    "shap.force_plot(base_values, shap_values, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e6bf1-d406-41a7-8c94-c29e4a5d1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating the Waterfall Plot\n",
    "\n",
    "explanation = shap.Explanation(values=shap_values , base_values=base_values, feature_names = feature_names)\n",
    "\n",
    "shap.waterfall_plot(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd76c2-0857-4a28-8b2f-e47c4802bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ba848-05b4-4285-8a0f-fb130d247c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset with a categorical column\n",
    "data = genotype\n",
    "df_data = pd.DataFrame(data)\n",
    "# Perform one-hot encoding on a specific categorical column\n",
    "encoded_df = pd.get_dummies(df_data)\n",
    "\n",
    "# Display the dataset after one-hot encoding\n",
    "print(\"\\nDataset after One-Hot Encoding:\")\n",
    "print(encoded_df)\n",
    "encoded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b79c10-3fdd-477e-873a-8243c47b41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample one-hot encoded dataset\n",
    "data = encoded_df\n",
    "df_data = pd.DataFrame(data)\n",
    "\n",
    "# Select features to set one-hot encoding to zero\n",
    "selected_features = less_important_features\n",
    "\n",
    "# Set one-hot encoding of selected features to zero\n",
    "for feature in selected_features:\n",
    "    for col in df_data.columns:\n",
    "        if feature in col:\n",
    "            df_data[col] = 0\n",
    "\n",
    "# Check the modified dataset\n",
    "print(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c54342-fca7-4e8a-89a7-7b9964c484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data['1642878_chrIV_282856_A_G'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34e72b-4d5d-4d69-a527-15205a0761c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "Y = multi_pheno.iloc[:, 17]#index=1 --> 1_YNB_1\n",
    "# Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "print(corrupted_X['7746462_chrXII_501434_T_G'])\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "print(type(corrupted_X_onehot))\n",
    "#print(corrupted_X['6810072_chrXI_231860_A_G'])\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205a8d5-081f-4f54-bfc5-227a6e0f2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Retrain the model on the new dataset**\n",
    "\n",
    "def model_build(pheno_name,num_hl,hl_list,hl_activation, out_activation, dropout_val,filters_, kernel_size_,stride_poolSize,Conv_Layers_Stride_Size):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    https://towardsdatascience.com/multi-output-model-with-tensorflow-keras-functional-api-875dd89aa7c6\n",
    "\n",
    "    def DNN_build(num_hl,hl_list,hl_activation, out_activation, dropout_val):\n",
    "    pheno_name = name of phenotype of interest\n",
    "    num_hl = number of hidden layers\n",
    "    hl_list = list of hidden layers\n",
    "    hl_activation = hidden layer activation function\n",
    "    out_activation = output layer activation function\n",
    "    dropout_val = Dropout value\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    input_layer = tf.keras.Input(shape=(28220,3))#(3138, 28220, 3)\n",
    "    \n",
    "    shared_convL1 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation,input_shape = (28220,3))(input_layer)\n",
    "    shared_convL1_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL1)\n",
    "    \n",
    "    shared_convL2 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation)(shared_convL1_max_pool)\n",
    "    shared_convL2_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL2)\n",
    "    \n",
    "    shared_convLayer_Flatten = tf.keras.layers.Flatten()(shared_convL2_max_pool)\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    kernel_regularizer_ = tf.keras.regularizers.L1L2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    shared_hidden_layer1 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[0], activation  = hl_activation)(shared_convLayer_Flatten)\n",
    "    shared_hidden_layer1_dp1 = tf.keras.layers.Dropout(dropout_val[0])(shared_hidden_layer1)\n",
    "\n",
    "    #model 1\n",
    "    model1_hidden_layer2 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[1], activation  = hl_activation)(shared_hidden_layer1_dp1)\n",
    "    model1_hidden_layer2_dp2 = tf.keras.layers.Dropout(dropout_val[1])(model1_hidden_layer2)\n",
    "\n",
    "    model1_hidden_layer3 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[2], activation  = hl_activation)(model1_hidden_layer2_dp2)\n",
    "    model1_hidden_layer3_dp3 = tf.keras.layers.Dropout(dropout_val[2])(model1_hidden_layer3)\n",
    "\n",
    "    model1_hidden_layer4 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[3], activation  = hl_activation)(model1_hidden_layer3_dp3)\n",
    "    model1_hidden_layer4_dp4 = tf.keras.layers.Dropout(dropout_val[3])(model1_hidden_layer4)\n",
    "    model1_hidden_layer4_dp4_fl = tf.keras.layers.Flatten()(model1_hidden_layer4_dp4)\n",
    "    \n",
    "    pheno_name_ = tf.keras.layers.Dense(units = 1, name = pheno_name, activation  = out_activation)(model1_hidden_layer4_dp4_fl)#1_CobaltChloride_1\n",
    "    model = tf.keras.models.Model(inputs = input_layer, outputs = [pheno_name_])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223944f-79cc-4e5b-9fb9-f9ab7f5e3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy','mse'])#change metrics to MSE\n",
    "def compile_model(pheno_name,model,loss_, learningRate, metrics_): #loss = 'sparse_categorical_crossentropy'\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest ['accuracy', 'mse']\n",
    "    '''\n",
    "\n",
    "    # Specify the optimizer, and compile the model with loss functions for both outputs\n",
    "    model.compile(\n",
    "       #optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate),#0.00027705 \n",
    "    optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learningRate, momentum=0.01, nesterov=True, name=\"SGD\"),\n",
    "      loss = {\n",
    "          pheno_name:loss_\n",
    "      }, \n",
    "      metrics = [metrics_]#{'CobaltChloride_1':[metrics_],'CopperSulfate_1':[metrics_],'Diamide_1':[metrics_]}\n",
    "      )#change metrics to MSE ##['accuracy','mse']\n",
    "    return model\n",
    "\n",
    "def runModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode,pheno_index):\n",
    "    '''\n",
    "    def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "    pheno_index: the index of the phenotype in y_train\n",
    "    '''\n",
    "    history = model.fit(\n",
    "    x_train, \n",
    "    [y_train[:]],\n",
    "    validation_data=(x_valid, [y_valid[:]]),\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c6a2c-3d3d-42f5-9c5d-9af6be022cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel_all(pheno_name, pheno_index,hl_list, drop_list):\n",
    "    model_mse = model_build(pheno_name,4,hl_list,'relu','linear',drop_list,8,7,2,3)#model initiallization\n",
    "    model_mse = compile_model(pheno_name,model_mse,tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mse\"),\n",
    "    0.00123695244,'mse')\n",
    "    history_mse =  runModel(model_mse,0.30,32,400,15, \"val_mse\", \"min\",pheno_index)\n",
    "    return history_mse,model_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c0605-1030-442e-8621-af5eee595b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mse,model_mse = runModel_all('1_YNB_1',17,[1464,608,264,146],[0.00,0.00,0.00,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ff809-b7fe-4551-b54e-7de153c744b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.save('important features/saved Models/1_YNB_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a858b-c6d7-4c95-82d8-4f7f20211c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model_mse.predict(x_valid)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "print(\"Mean Squared Error on the validation set:\", mse)\n",
    "\n",
    "# Make predictions on the tesing set\n",
    "y_pred = model_mse.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on the testing set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd899e2b-78b1-466f-bbe6-2e12e81ee0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is for LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "model_mse= tf.keras.models.load_model('1_YNB_1.h5')\n",
    "print(type(genotype))\n",
    "genotype_data = genotype.to_numpy()\n",
    "class_names = ['Positive','Negative']\n",
    "my_features = genotype.columns.tolist()\n",
    "#model_mse_ = tf.keras.models.load_model('important features/Models/1_CobaltChloride_1.h5', custom_objects=custom_objects, compile=False)\n",
    "\n",
    "explainer = lime.lime_tabular.RecurrentTabularExplainer(x_train, mode='regression', training_labels=None, feature_names=my_features, \n",
    "                        categorical_features=None, categorical_names=None, kernel_width=None, kernel=None, verbose=False, \n",
    "                        class_names=class_names, feature_selection='auto', discretize_continuous=True, discretizer='quartile', \n",
    "                         random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a3b5b-c1b2-4180-b406-aa3d7f9ee61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "instance = x_test[i]\n",
    "exp = explainer.explain_instance(instance, model_mse.predict, num_features = len(my_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf298fa-1670-4f60-b91b-3d65a448fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()\n",
    "lime_importance = dict(exp.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededc172-3b90-44f3-8937-4f4c5a08fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_lime_importance = dict(sorted(lime_importance.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_lime_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7b853-1a2a-4be4-916b-f19c80eff516",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_important_lime = [k for k, v in sorted_lime_importance.items() if v <= 1e-5]\n",
    "len(less_important_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88033d24-a6d4-4bec-a922-e3b81d972d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_lime = [k for k, v in lime_importance.items() if v >= 1e-5]\n",
    "len(important_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d27072-0bbf-43b5-9a27-e6da8dca0bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(main_list, search_list):\n",
    "    return [i for i, x in enumerate(main_list) if x in search_list]\n",
    "\n",
    "indexes_lime = find_indexes(lime_importance, important_lime)\n",
    "len(indexes_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ef0cd-012f-46e2-b3a1-286b77a66ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(main_list, search_list):\n",
    "    return [i for i, x in enumerate(main_list) if x in search_list]\n",
    "\n",
    "indexes = find_indexes(lime_importance, less_important_lime)\n",
    "print(indexes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af84b6a-7d26-4904-bd93-34c3114dd36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_lime_importance_list = sorted(sorted_lime_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "num_features_to_plot = min(20, len(sorted_lime_importance_list))\n",
    "\n",
    "# Extract feature names and importance values for visualization\n",
    "sorted_lime_names = [x[0] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "sorted_lime_importance_ = [x[1] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "\n",
    "# Plot the ranked features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_lime_names, sorted_lime_importance_)\n",
    "plt.xlabel('LIME Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Ranked Features based on LIME Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42fb59-17ea-4c45-991e-57f151e50797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_lime_importance_list = sorted(sorted_lime_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "num_features_to_plot = min(20, len(sorted_lime_importance_list))\n",
    "\n",
    "# Extract feature names and importance values for visualization\n",
    "sorted_lime_names = [x[0] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "sorted_lime_importance_ = [x[1] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(sorted_lime_importance_, sorted_lime_names, color='green', s=100, alpha=0.6)\n",
    "plt.xlabel('LIME Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature with their corresponding SHAP value')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322de13-8ddb-489a-bb74-91e9b7740b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset with a categorical column\n",
    "data_lime = genotype\n",
    "df_data = pd.DataFrame(data_lime)\n",
    "# Perform one-hot encoding on a specific categorical column\n",
    "encoded_df = pd.get_dummies(df_data)\n",
    "\n",
    "# Display the dataset after one-hot encoding\n",
    "print(\"\\nDataset after One-Hot Encoding:\")\n",
    "print(encoded_df)\n",
    "encoded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788f933-695c-4c3c-a5b3-831653efad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample one-hot encoded dataset\n",
    "data = encoded_df\n",
    "df_data_ = pd.DataFrame(data)\n",
    "type(df_data.columns)\n",
    "# Select features to set one-hot encoding to zero\n",
    "selected_features = indexes\n",
    "\n",
    "# Set one-hot encoding of selected features to zero\n",
    "for index in selected_features:\n",
    "    for col in df_data_.columns:\n",
    "        if index in df_data_.columns:\n",
    "            df_data_[col] = 0\n",
    "\n",
    "# Check the modified dataset\n",
    "print(df_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93404183-fe77-4fa6-bb88-2a7cf72daaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data_\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "Y = multi_pheno.iloc[:, 17]#index=1 --> 1_YNB_1\n",
    "# Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "print(type(corrupted_X_onehot))\n",
    "#print(corrupted_X['6810072_chrXI_231860_A_G'])\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44988060-07ad-411a-b332-b71e48ae0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Retrain the model on the new dataset**\n",
    "\n",
    "def model_build(pheno_name,num_hl,hl_list,hl_activation, out_activation, dropout_val,filters_, kernel_size_,stride_poolSize,Conv_Layers_Stride_Size):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    https://towardsdatascience.com/multi-output-model-with-tensorflow-keras-functional-api-875dd89aa7c6\n",
    "\n",
    "    def DNN_build(num_hl,hl_list,hl_activation, out_activation, dropout_val):\n",
    "    pheno_name = name of phenotype of interest\n",
    "    num_hl = number of hidden layers\n",
    "    hl_list = list of hidden layers\n",
    "    hl_activation = hidden layer activation function\n",
    "    out_activation = output layer activation function\n",
    "    dropout_val = Dropout value\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    input_layer = tf.keras.Input(shape=(28220,3))#(3138, 28220, 3)\n",
    "    \n",
    "    shared_convL1 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation,input_shape = (28220,3))(input_layer)\n",
    "    shared_convL1_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL1)\n",
    "    \n",
    "    shared_convL2 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation)(shared_convL1_max_pool)\n",
    "    shared_convL2_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL2)\n",
    "    \n",
    "    shared_convLayer_Flatten = tf.keras.layers.Flatten()(shared_convL2_max_pool)\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    kernel_regularizer_ = tf.keras.regularizers.L1L2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    shared_hidden_layer1 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[0], activation  = hl_activation)(shared_convLayer_Flatten)\n",
    "    shared_hidden_layer1_dp1 = tf.keras.layers.Dropout(dropout_val[0])(shared_hidden_layer1)\n",
    "\n",
    "    #model 1\n",
    "    model1_hidden_layer2 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[1], activation  = hl_activation)(shared_hidden_layer1_dp1)\n",
    "    model1_hidden_layer2_dp2 = tf.keras.layers.Dropout(dropout_val[1])(model1_hidden_layer2)\n",
    "\n",
    "    model1_hidden_layer3 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[2], activation  = hl_activation)(model1_hidden_layer2_dp2)\n",
    "    model1_hidden_layer3_dp3 = tf.keras.layers.Dropout(dropout_val[2])(model1_hidden_layer3)\n",
    "\n",
    "    model1_hidden_layer4 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[3], activation  = hl_activation)(model1_hidden_layer3_dp3)\n",
    "    model1_hidden_layer4_dp4 = tf.keras.layers.Dropout(dropout_val[3])(model1_hidden_layer4)\n",
    "    model1_hidden_layer4_dp4_fl = tf.keras.layers.Flatten()(model1_hidden_layer4_dp4)\n",
    "    \n",
    "    pheno_name_ = tf.keras.layers.Dense(units = 1, name = pheno_name, activation  = out_activation)(model1_hidden_layer4_dp4_fl)#1_CobaltChloride_1\n",
    "    model = tf.keras.models.Model(inputs = input_layer, outputs = [pheno_name_])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddf6f8-2444-461d-acd1-6c184c75aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy','mse'])#change metrics to MSE\n",
    "def compile_model(pheno_name,model,loss_, learningRate, metrics_): #loss = 'sparse_categorical_crossentropy'\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest ['accuracy', 'mse']\n",
    "    '''\n",
    "\n",
    "    # Specify the optimizer, and compile the model with loss functions for both outputs\n",
    "    model.compile(\n",
    "       #optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate),#0.00027705 \n",
    "    optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learningRate, momentum=0.01, nesterov=True, name=\"SGD\"),\n",
    "      loss = {\n",
    "          pheno_name:loss_\n",
    "      }, \n",
    "      metrics = [metrics_]#{'CobaltChloride_1':[metrics_],'CopperSulfate_1':[metrics_],'Diamide_1':[metrics_]}\n",
    "      )#change metrics to MSE ##['accuracy','mse']\n",
    "    return model\n",
    "\n",
    "def runModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode,pheno_index):\n",
    "    '''\n",
    "    def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "    pheno_index: the index of the phenotype in y_train\n",
    "    '''\n",
    "    history = model.fit(\n",
    "    x_train, \n",
    "    [y_train[:]],\n",
    "    validation_data=(x_valid, [y_valid[:]]),\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5fb8e-0cf0-4eac-ba48-6647e8f3c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel_all(pheno_name, pheno_index,hl_list, drop_list):\n",
    "    model_mse = model_build(pheno_name,4,hl_list,'relu','linear',drop_list,8,7,2,3)#model initiallization\n",
    "    model_mse = compile_model(pheno_name,model_mse,tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mse\"),\n",
    "    0.00123695244,'mse')\n",
    "    history_mse =  runModel(model_mse,0.30,32,400,15, \"val_mse\", \"min\",pheno_index)\n",
    "    return history_mse,model_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2043e28d-434b-4100-a173-342c85ee594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mse,model_mse = runModel_all('1_YNB_1',17,[1464,608,264,146],[0.00,0.00,0.00,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dabaf5-374d-43ca-baa4-722c30cb320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model_mse.predict(x_valid)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "print(\"Mean Squared Error on the validation set:\", mse)\n",
    "\n",
    "# Make predictions on the tesing set\n",
    "y_pred = model_mse.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on the testing set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c99f98-8883-4f90-9995-cf415f5cca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Intersection of LIME and SHAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e5196-3181-48cd-8ac2-091bfced5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_indexes(lime_indexes, shap_indexes):\n",
    "    # Convert lists to sets\n",
    "    lime_set = set(lime_indexes)\n",
    "    shap_set = set(shap_indexes)\n",
    "    \n",
    "    # Find the intersection\n",
    "    common_indexes = lime_set.intersection(shap_set)\n",
    "    \n",
    "    return common_indexes\n",
    "    \n",
    "\n",
    "common_indexes = find_common_indexes(indexes_lime, indexes_shap)\n",
    "print(type(common_indexes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99758c-9870-4f6d-bea8-b97e6b6124c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indexes = np.array(list(common_indexes))\n",
    "(common_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5910b3-1bfb-40bc-96dd-0280492690f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indexes_ = common_indexes.tolist()\n",
    "len(common_indexes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe93c6-409d-4168-8051-8eeeb9833886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strings_at_indices(strings, indices):\n",
    "\n",
    "    valid_indices = [index for index in indices if 0 <= index < len(strings)]\n",
    "    \n",
    "    # Get strings at valid indices\n",
    "    result = [strings[index] for index in valid_indices]\n",
    "    \n",
    "    return result\n",
    "\n",
    "matching_features = get_strings_at_indices(feature_importance_df['Feature'].tolist(), common_indexes_)\n",
    "len(matching_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47398a-839b-4194-8ac4-d6b56004d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genotypeFile = 'genotype.csv'\n",
    "genotype = pd.read_csv(genotypeFile, sep = '\\t', index_col = 0)\n",
    "print('genotypeFile shape:', genotype.shape )\n",
    "\n",
    "phenotypeFile = 'phenotype.csv'\n",
    "multi_pheno = pd.read_csv(phenotypeFile, sep = ',', index_col = 0)\n",
    "print('Phenotype_Multi shape:', multi_pheno.shape )\n",
    "\n",
    "\n",
    "# take a small part to test code\n",
    "# genotype\n",
    "X = genotype\n",
    "\n",
    "mask = ~X.columns.str.contains('|'.join(matching_features))\n",
    "\n",
    "# Set values to zero for non-matching columns\n",
    "X.loc[:, mask] = 0\n",
    "\n",
    "#display(X.head())\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "\n",
    "#Chage the index for each model here accoding to the correspinding model\n",
    "Y = multi_pheno.iloc[:, 17]#index=0 --> 1_YNB_1\n",
    "# # Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "# corrupted_X.head()\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "# corrupted_X_onehot.shape\n",
    "\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c82f97-7ea9-475a-8d19-9411189efc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Retrain the model on the new dataset**\n",
    "\n",
    "def model_build(pheno_name,num_hl,hl_list,hl_activation, out_activation, dropout_val,filters_, kernel_size_,stride_poolSize,Conv_Layers_Stride_Size):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    https://towardsdatascience.com/multi-output-model-with-tensorflow-keras-functional-api-875dd89aa7c6\n",
    "\n",
    "    def DNN_build(num_hl,hl_list,hl_activation, out_activation, dropout_val):\n",
    "    pheno_name = name of phenotype of interest\n",
    "    num_hl = number of hidden layers\n",
    "    hl_list = list of hidden layers\n",
    "    hl_activation = hidden layer activation function\n",
    "    out_activation = output layer activation function\n",
    "    dropout_val = Dropout value\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    input_layer = tf.keras.Input(shape=(28220,3))#(3138, 28220, 3)\n",
    "    \n",
    "    shared_convL1 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation,input_shape = (28220,3))(input_layer)\n",
    "    shared_convL1_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL1)\n",
    "    \n",
    "    shared_convL2 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation)(shared_convL1_max_pool)\n",
    "    shared_convL2_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL2)\n",
    "    \n",
    "    shared_convLayer_Flatten = tf.keras.layers.Flatten()(shared_convL2_max_pool)\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    kernel_regularizer_ = tf.keras.regularizers.L1L2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    shared_hidden_layer1 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[0], activation  = hl_activation)(shared_convLayer_Flatten)\n",
    "    shared_hidden_layer1_dp1 = tf.keras.layers.Dropout(dropout_val[0])(shared_hidden_layer1)\n",
    "\n",
    "    #model 1\n",
    "    model1_hidden_layer2 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[1], activation  = hl_activation)(shared_hidden_layer1_dp1)\n",
    "    model1_hidden_layer2_dp2 = tf.keras.layers.Dropout(dropout_val[1])(model1_hidden_layer2)\n",
    "\n",
    "    model1_hidden_layer3 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[2], activation  = hl_activation)(model1_hidden_layer2_dp2)\n",
    "    model1_hidden_layer3_dp3 = tf.keras.layers.Dropout(dropout_val[2])(model1_hidden_layer3)\n",
    "\n",
    "    model1_hidden_layer4 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[3], activation  = hl_activation)(model1_hidden_layer3_dp3)\n",
    "    model1_hidden_layer4_dp4 = tf.keras.layers.Dropout(dropout_val[3])(model1_hidden_layer4)\n",
    "    model1_hidden_layer4_dp4_fl = tf.keras.layers.Flatten()(model1_hidden_layer4_dp4)\n",
    "    \n",
    "    pheno_name_ = tf.keras.layers.Dense(units = 1, name = pheno_name, activation  = out_activation)(model1_hidden_layer4_dp4_fl)#1_CobaltChloride_1\n",
    "    model = tf.keras.models.Model(inputs = input_layer, outputs = [pheno_name_])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea0c7c-fc59-4f57-887c-74b561350b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy','mse'])#change metrics to MSE\n",
    "def compile_model(pheno_name,model,loss_, learningRate, metrics_): #loss = 'sparse_categorical_crossentropy'\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest ['accuracy', 'mse']\n",
    "    '''\n",
    "\n",
    "    # Specify the optimizer, and compile the model with loss functions for both outputs\n",
    "    model.compile(\n",
    "       #optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate),#0.00027705 \n",
    "    optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learningRate, momentum=0.01, nesterov=True, name=\"SGD\"),\n",
    "      loss = {\n",
    "          pheno_name:loss_\n",
    "      }, \n",
    "      metrics = [metrics_]#{'CobaltChloride_1':[metrics_],'CopperSulfate_1':[metrics_],'Diamide_1':[metrics_]}\n",
    "      )#change metrics to MSE ##['accuracy','mse']\n",
    "    return model\n",
    "\n",
    "def runModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode,pheno_index):\n",
    "    '''\n",
    "    def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "    pheno_index: the index of the phenotype in y_train\n",
    "    '''\n",
    "    history = model.fit(\n",
    "    x_train, \n",
    "    [y_train[:]],\n",
    "    validation_data=(x_valid, [y_valid[:]]),\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a99c1-1b39-490f-8736-c84d62705529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel_all(pheno_name, pheno_index,hl_list, drop_list):\n",
    "    model_mse = model_build(pheno_name,4,hl_list,'relu','linear',drop_list,8,7,2,3)#model initiallization\n",
    "    model_mse = compile_model(pheno_name,model_mse,tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mse\"),\n",
    "    0.00123695244,'mse')\n",
    "    history_mse =  runModel(model_mse,0.30,32,400,15, \"val_mse\", \"min\",pheno_index)\n",
    "    return history_mse,model_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0847ef-d440-478a-93d5-8e31365b4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mse,model_mse = runModel_all('1_YNB_1',17,[1464,608,264,146],[0.00,0.00,0.00,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffe543-1cd4-404c-a849-2ac4ed665dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model_mse.predict(x_valid)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "print(\"Mean Squared Error on the validation set:\", mse)\n",
    "\n",
    "# Make predictions on the tesing set\n",
    "y_pred = model_mse.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on the testing set:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
