{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "#!pip install keras-tuner\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.layers import Layer\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "from matplotlib import pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow.python.ops.numpy_ops import np_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "genotypeFile = 'genotype.csv'\n",
    "genotype = pd.read_csv(genotypeFile, sep = '\\t', index_col = 0)\n",
    "print('genotypeFile shape:', genotype.shape )\n",
    "\n",
    "phenotypeFile = 'phenotype.csv'\n",
    "multi_pheno = pd.read_csv(phenotypeFile, sep = ',', index_col = 0)\n",
    "print('Phenotype_Multi shape:', multi_pheno.shape )\n",
    "\n",
    "\n",
    "# take a small part to test code\n",
    "# genotype\n",
    "X = genotype\n",
    "#display(X.head())\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "\n",
    "#Chage the index for each model here accoding to the correspinding model\n",
    "Y = multi_pheno.iloc[:, 18]#index=1 --> 1_YPD_1\n",
    "# Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "# corrupted_X.head()\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "# corrupted_X_onehot.shape\n",
    "\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430eae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_list = list([i for i in range(404)])[0:100]\n",
    "if len(background_list)==0:\n",
    "    background_list = x_train[np.random.choice(x_train.shape[0],100, replace=False)]\n",
    "background = x_train[background_list]\n",
    "background.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from shap import initjs\n",
    "initjs()\n",
    "custom_objects = None\n",
    "# Define custom objects if necessary\n",
    "custom_objects = None\n",
    "\n",
    "# Load the model with specified reduction strategy\n",
    "model_mse = tf.keras.models.load_model('1_YPD_1.h5', custom_objects=custom_objects, compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44320628-e5ae-4717-9876-75eca5078202",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model_mse,x_test)\n",
    "base_value = explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb75cd-2e06-4a09-acd0-d94e2afa35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_values = base_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219316d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(x_test,check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0b64f-5e24-490f-8a10-d2fc6a385d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shap_values = np.mean(shap_values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0abd94-2e3f-49fb-ad9c-1632262620f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_shap_values = np.abs(shap_values[0])\n",
    "feature_importance = np.mean(abs_shap_values, axis=0)\n",
    "len(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0017c9f-897b-4daa-9f84-df6a8c2c3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42dfc0e-532f-490d-9478-cbd585e5217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_impportance_sum = feature_importance.sum(axis=1)\n",
    "feature_impportance_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca145b-7491-4171-a690-7a5e217cdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns.tolist(),  # Convert X.columns to a list\n",
    "    'Importance': feature_impportance_sum.tolist()\n",
    "})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31125c14-97c8-4241-b9de-115d2daae30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = feature_importance_df[feature_importance_df['Importance']  <= 1e-4]\n",
    "less_important_features = num.Feature.tolist()\n",
    "less_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729cbc1-22f1-4392-ad3c-18de9b86dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = feature_importance_df[feature_importance_df['Importance']  >= 1e-4]\n",
    "important_features = num.Feature.tolist()\n",
    "len(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55a15a-9099-414f-a058-a55ce6a0596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(main_list, search_list):\n",
    "    return [i for i, x in enumerate(main_list) if x in search_list]\n",
    "\n",
    "indexes_shap = find_indexes(feature_importance_df['Feature'].tolist(), important_features)\n",
    "print(len(indexes_shap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165763af-f52f-4e56-9cff-dbb0a3d05251",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_dict = dict(zip(feature_importance_df['Feature'], feature_importance_df['Importance']))\n",
    "feature_importance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff8924-5331-43fd-a508-613700b86495",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#### First we want to remove all values that are less than and equal to zero and save them in a list\n",
    "\n",
    "removed_pairs_features = []\n",
    "\n",
    "filtered_pairs_features = [(key, value) for key, value in sorted_features if value > 0]\n",
    "removed_pairs_features = [(key, value) for key, value in sorted_features if value <= 0]\n",
    "filtered_pairs_features_numpy = np.array(filtered_pairs_features)\n",
    "# print(\"Filtered key-value pairs:\", filtered_pairs_features)\n",
    "# print(\"Removed key-value pairs:\", removed_pairs_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66452a48-1405-4832-8052-e59f6bdc5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature names and importance values for visualization\n",
    "sorted_feature_names = [x[0] for x in filtered_pairs_features[:20]]\n",
    "sorted_feature_importance = [x[1] for x in filtered_pairs_features[:20]]\n",
    "\n",
    "# Plot the ranked features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_feature_names, sorted_feature_importance)\n",
    "plt.xlabel('SHAP Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Ranked Features based on SHAP Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50366b-8da6-443e-b0e1-3b3761ce9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_shap_values = [pair[1] for pair in filtered_pairs_features[:30]]\n",
    "feature_shap_values_names = [pair[0] for pair in filtered_pairs_features[:30]]\n",
    "feature_names = np.array(feature_shap_values_names)\n",
    "type(feature_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f026fa-e233-4c5d-a8f4-cd65c938c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a list of features and their corresponding SHAP values\n",
    "features = feature_importance_df['Feature'][:20]\n",
    "shap_values = feature_importance_df['Importance'][:20]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(shap_values, features, color='green', s=100, alpha=0.6)\n",
    "plt.xlabel('SHAP Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature with their corresponding SHAP value')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7dce96-e7af-42e5-a55a-faf0ac73811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating the FORCE Plot\n",
    "shap_values = np.array(feature_shap_values)  # Assuming 100 samples and 5 features\n",
    "shap_values_ = np.reshape(shap_values, (-1,2))\n",
    "shap.force_plot(base_values, shap_values, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e6bf1-d406-41a7-8c94-c29e4a5d1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating the Waterfall Plot\n",
    "\n",
    "explanation = shap.Explanation(values=shap_values , base_values=base_values, feature_names = feature_names)\n",
    "\n",
    "shap.waterfall_plot(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd76c2-0857-4a28-8b2f-e47c4802bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ba848-05b4-4285-8a0f-fb130d247c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset with a categorical column\n",
    "data = genotype\n",
    "df_data = pd.DataFrame(data)\n",
    "# Perform one-hot encoding on a specific categorical column\n",
    "encoded_df = pd.get_dummies(df_data)\n",
    "\n",
    "# Display the dataset after one-hot encoding\n",
    "print(\"\\nDataset after One-Hot Encoding:\")\n",
    "print(encoded_df)\n",
    "encoded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b79c10-3fdd-477e-873a-8243c47b41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample one-hot encoded dataset\n",
    "data = encoded_df\n",
    "df_data = pd.DataFrame(data)\n",
    "\n",
    "# Select features to set one-hot encoding to zero\n",
    "selected_features = less_important_features\n",
    "\n",
    "# Set one-hot encoding of selected features to zero\n",
    "for feature in selected_features:\n",
    "    for col in df_data.columns:\n",
    "        if feature in col:\n",
    "            df_data[col] = 0\n",
    "\n",
    "# Check the modified dataset\n",
    "print(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c54342-fca7-4e8a-89a7-7b9964c484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the one-hot encoding is working for less important features\n",
    "print(df_data['1461630_chrIV_101608_G_A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447003f2-2e31-4392-b6ce-59ec4637f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "Y = multi_pheno.iloc[:, 18]#index=1 --> 1_YNB_1\n",
    "# Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "print(corrupted_X['7746462_chrXII_501434_T_G'])\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "print(type(corrupted_X_onehot))\n",
    "#print(corrupted_X['6810072_chrXI_231860_A_G'])\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807be09-43a0-4573-80c3-8c3e57816526",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Retrain the model on the new dataset**\n",
    "\n",
    "def model_build(pheno_name,num_hl,hl_list,hl_activation, out_activation, dropout_val,filters_, kernel_size_,stride_poolSize,Conv_Layers_Stride_Size):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    https://towardsdatascience.com/multi-output-model-with-tensorflow-keras-functional-api-875dd89aa7c6\n",
    "\n",
    "    def DNN_build(num_hl,hl_list,hl_activation, out_activation, dropout_val):\n",
    "    pheno_name = name of phenotype of interest\n",
    "    num_hl = number of hidden layers\n",
    "    hl_list = list of hidden layers\n",
    "    hl_activation = hidden layer activation function\n",
    "    out_activation = output layer activation function\n",
    "    dropout_val = Dropout value\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    input_layer = tf.keras.Input(shape=(28220,3))#(3138, 28220, 3)\n",
    "    \n",
    "    shared_convL1 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation,input_shape = (28220,3))(input_layer)\n",
    "    shared_convL1_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL1)\n",
    "    \n",
    "    shared_convL2 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation)(shared_convL1_max_pool)\n",
    "    shared_convL2_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL2)\n",
    "    \n",
    "    shared_convLayer_Flatten = tf.keras.layers.Flatten()(shared_convL2_max_pool)\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    kernel_regularizer_ = tf.keras.regularizers.L1L2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    shared_hidden_layer1 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[0], activation  = hl_activation)(shared_convLayer_Flatten)\n",
    "    shared_hidden_layer1_dp1 = tf.keras.layers.Dropout(dropout_val[0])(shared_hidden_layer1)\n",
    "\n",
    "    #model 1\n",
    "    model1_hidden_layer2 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[1], activation  = hl_activation)(shared_hidden_layer1_dp1)\n",
    "    model1_hidden_layer2_dp2 = tf.keras.layers.Dropout(dropout_val[1])(model1_hidden_layer2)\n",
    "\n",
    "    model1_hidden_layer3 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[2], activation  = hl_activation)(model1_hidden_layer2_dp2)\n",
    "    model1_hidden_layer3_dp3 = tf.keras.layers.Dropout(dropout_val[2])(model1_hidden_layer3)\n",
    "\n",
    "    model1_hidden_layer4 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[3], activation  = hl_activation)(model1_hidden_layer3_dp3)\n",
    "    model1_hidden_layer4_dp4 = tf.keras.layers.Dropout(dropout_val[3])(model1_hidden_layer4)\n",
    "    model1_hidden_layer4_dp4_fl = tf.keras.layers.Flatten()(model1_hidden_layer4_dp4)\n",
    "    \n",
    "    pheno_name_ = tf.keras.layers.Dense(units = 1, name = pheno_name, activation  = out_activation)(model1_hidden_layer4_dp4_fl)#1_CobaltChloride_1\n",
    "    model = tf.keras.models.Model(inputs = input_layer, outputs = [pheno_name_])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1114b47-c85d-4d5f-99d4-d8b2cc22e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy','mse'])#change metrics to MSE\n",
    "def compile_model(pheno_name,model,loss_, learningRate, metrics_): #loss = 'sparse_categorical_crossentropy'\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest ['accuracy', 'mse']\n",
    "    '''\n",
    "\n",
    "    # Specify the optimizer, and compile the model with loss functions for both outputs\n",
    "    model.compile(\n",
    "       #optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate),#0.00027705 \n",
    "    optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learningRate, momentum=0.01, nesterov=True, name=\"SGD\"),\n",
    "      loss = {\n",
    "          pheno_name:loss_\n",
    "      }, \n",
    "      metrics = [metrics_]#{'CobaltChloride_1':[metrics_],'CopperSulfate_1':[metrics_],'Diamide_1':[metrics_]}\n",
    "      )#change metrics to MSE ##['accuracy','mse']\n",
    "    return model\n",
    "\n",
    "def runModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode,pheno_index):\n",
    "    '''\n",
    "    def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "    pheno_index: the index of the phenotype in y_train\n",
    "    '''\n",
    "    history = model.fit(\n",
    "    x_train, \n",
    "    [y_train[:]],\n",
    "    validation_data=(x_valid, [y_valid[:]]),\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f1c1e-d156-4c8b-84fa-d0361121e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel_all(pheno_name, pheno_index,hl_list, drop_list):\n",
    "    model_mse = model_build(pheno_name,4,hl_list,'relu','linear',drop_list,8,7,2,3)#model initiallization\n",
    "    model_mse = compile_model(pheno_name,model_mse,tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mse\"),\n",
    "    0.00123695244,'mse')\n",
    "    history_mse =  runModel(model_mse,0.30,32,400,15, \"val_mse\", \"min\",pheno_index)\n",
    "    return history_mse,model_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71e1f4-610b-4be5-88ba-f846e2e54c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mse,model_mse = runModel_all('1_YPD_1',18,[1464,608,264,146],[0.00,0.00,0.00,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8dd5b0-8f6c-4dc3-9800-5225953bdc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.save('important features/saved Models/1_YPD_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74a86a-ab17-4392-8dbf-c1a82320f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model_mse.predict(x_valid)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "print(\"Mean Squared Error on the validation set:\", mse)\n",
    "\n",
    "# Make predictions on the tesing set\n",
    "y_pred = model_mse.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on the testing set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a217ca-fa5f-4c59-a491-e644daa6984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is for LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "model_mse= tf.keras.models.load_model('1_YPD_1.h5')\n",
    "print(type(genotype))\n",
    "genotype_data = genotype.to_numpy()\n",
    "class_names = ['Positive','Negative']\n",
    "my_features = genotype.columns.tolist()\n",
    "#model_mse_ = tf.keras.models.load_model('important features/Models/1_CobaltChloride_1.h5', custom_objects=custom_objects, compile=False)\n",
    "\n",
    "explainer = lime.lime_tabular.RecurrentTabularExplainer(x_train, mode='regression', training_labels=None, feature_names=my_features, \n",
    "                        categorical_features=None, categorical_names=None, kernel_width=None, kernel=None, verbose=False, \n",
    "                        class_names=class_names, feature_selection='auto', discretize_continuous=True, discretizer='quartile', \n",
    "                         random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b71a8-edda-4149-a8e3-6c7e628a7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "instance = x_test[i]\n",
    "exp = explainer.explain_instance(instance, model_mse.predict, num_features = len(my_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04644f-2b54-44a0-bafa-e000b95b1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()\n",
    "lime_importance = dict(exp.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8936f-eb0b-407f-a93f-d1067318f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_lime_importance = dict(sorted(lime_importance.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_lime_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04536616-ff0d-4f46-8bf6-60c8ba6352b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_important_lime = [k for k, v in sorted_lime_importance.items() if v <= 1e-5]\n",
    "len(less_important_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b900211-f7fb-47f6-a3ca-d5a3b26f6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_lime = [k for k, v in lime_importance.items() if v >= 1e-5]\n",
    "len(important_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756311f4-74c5-4799-ac15-a46beeb2ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(main_list, search_list):\n",
    "    return [i for i, x in enumerate(main_list) if x in search_list]\n",
    "\n",
    "indexes_lime = find_indexes(lime_importance, important_lime)\n",
    "len(indexes_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad545e-b584-413a-a7f8-74b97e97af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(main_list, search_list):\n",
    "    return [i for i, x in enumerate(main_list) if x in search_list]\n",
    "\n",
    "indexes = find_indexes(lime_importance, less_important_lime)\n",
    "print(indexes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfad8b-e0f5-45be-98bd-cba0bdb36bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_lime_importance_list = sorted(sorted_lime_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "num_features_to_plot = min(20, len(sorted_lime_importance_list))\n",
    "\n",
    "# Extract feature names and importance values for visualization\n",
    "sorted_lime_names = [x[0] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "sorted_lime_importance_ = [x[1] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "\n",
    "# Plot the ranked features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_lime_names, sorted_lime_importance_)\n",
    "plt.xlabel('LIME Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Ranked Features based on LIME Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cb58c-ff5d-4654-9d01-aec9ca209531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_lime_importance_list = sorted(sorted_lime_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "num_features_to_plot = min(20, len(sorted_lime_importance_list))\n",
    "\n",
    "# Extract feature names and importance values for visualization\n",
    "sorted_lime_names = [x[0] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "sorted_lime_importance_ = [x[1] for x in sorted_lime_importance_list[:num_features_to_plot]]\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(sorted_lime_importance_, sorted_lime_names, color='green', s=100, alpha=0.6)\n",
    "plt.xlabel('LIME Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature with their corresponding SHAP value')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da447173-55ca-4155-b60d-ddaeecb94e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset with a categorical column\n",
    "data_lime = genotype\n",
    "df_data = pd.DataFrame(data_lime)\n",
    "# Perform one-hot encoding on a specific categorical column\n",
    "encoded_df = pd.get_dummies(df_data)\n",
    "\n",
    "# Display the dataset after one-hot encoding\n",
    "print(\"\\nDataset after One-Hot Encoding:\")\n",
    "print(encoded_df)\n",
    "encoded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083ed45-894c-4dbf-940e-8306ac59490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample one-hot encoded dataset\n",
    "data = encoded_df\n",
    "df_data_ = pd.DataFrame(data)\n",
    "type(df_data.columns)\n",
    "# Select features to set one-hot encoding to zero\n",
    "selected_features = indexes\n",
    "\n",
    "# Set one-hot encoding of selected features to zero\n",
    "for index in selected_features:\n",
    "    for col in df_data_.columns:\n",
    "        if index in df_data_.columns:\n",
    "            df_data_[col] = 0\n",
    "\n",
    "# Check the modified dataset\n",
    "print(df_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd89d6-0ec3-4ab1-b5f2-871252519831",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data_\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "Y = multi_pheno.iloc[:, 18]#index=1 --> 1_YPD_1\n",
    "# Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "print(type(corrupted_X_onehot))\n",
    "#print(corrupted_X['6810072_chrXI_231860_A_G'])\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061a617-7162-42f1-b055-b5dc7b704a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Retrain the model on the new dataset**\n",
    "\n",
    "def model_build(pheno_name,num_hl,hl_list,hl_activation, out_activation, dropout_val,filters_, kernel_size_,stride_poolSize,Conv_Layers_Stride_Size):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    https://towardsdatascience.com/multi-output-model-with-tensorflow-keras-functional-api-875dd89aa7c6\n",
    "\n",
    "    def DNN_build(num_hl,hl_list,hl_activation, out_activation, dropout_val):\n",
    "    pheno_name = name of phenotype of interest\n",
    "    num_hl = number of hidden layers\n",
    "    hl_list = list of hidden layers\n",
    "    hl_activation = hidden layer activation function\n",
    "    out_activation = output layer activation function\n",
    "    dropout_val = Dropout value\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    input_layer = tf.keras.Input(shape=(28220,3))#(3138, 28220, 3)\n",
    "    \n",
    "    shared_convL1 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation,input_shape = (28220,3))(input_layer)\n",
    "    shared_convL1_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL1)\n",
    "    \n",
    "    shared_convL2 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation)(shared_convL1_max_pool)\n",
    "    shared_convL2_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL2)\n",
    "    \n",
    "    shared_convLayer_Flatten = tf.keras.layers.Flatten()(shared_convL2_max_pool)\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    kernel_regularizer_ = tf.keras.regularizers.L1L2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    shared_hidden_layer1 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[0], activation  = hl_activation)(shared_convLayer_Flatten)\n",
    "    shared_hidden_layer1_dp1 = tf.keras.layers.Dropout(dropout_val[0])(shared_hidden_layer1)\n",
    "\n",
    "    #model 1\n",
    "    model1_hidden_layer2 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[1], activation  = hl_activation)(shared_hidden_layer1_dp1)\n",
    "    model1_hidden_layer2_dp2 = tf.keras.layers.Dropout(dropout_val[1])(model1_hidden_layer2)\n",
    "\n",
    "    model1_hidden_layer3 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[2], activation  = hl_activation)(model1_hidden_layer2_dp2)\n",
    "    model1_hidden_layer3_dp3 = tf.keras.layers.Dropout(dropout_val[2])(model1_hidden_layer3)\n",
    "\n",
    "    model1_hidden_layer4 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[3], activation  = hl_activation)(model1_hidden_layer3_dp3)\n",
    "    model1_hidden_layer4_dp4 = tf.keras.layers.Dropout(dropout_val[3])(model1_hidden_layer4)\n",
    "    model1_hidden_layer4_dp4_fl = tf.keras.layers.Flatten()(model1_hidden_layer4_dp4)\n",
    "    \n",
    "    pheno_name_ = tf.keras.layers.Dense(units = 1, name = pheno_name, activation  = out_activation)(model1_hidden_layer4_dp4_fl)#1_CobaltChloride_1\n",
    "    model = tf.keras.models.Model(inputs = input_layer, outputs = [pheno_name_])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ff154-c576-49c3-9c6e-8083f17f0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy','mse'])#change metrics to MSE\n",
    "def compile_model(pheno_name,model,loss_, learningRate, metrics_): #loss = 'sparse_categorical_crossentropy'\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest ['accuracy', 'mse']\n",
    "    '''\n",
    "\n",
    "    # Specify the optimizer, and compile the model with loss functions for both outputs\n",
    "    model.compile(\n",
    "       #optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate),#0.00027705 \n",
    "    optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learningRate, momentum=0.01, nesterov=True, name=\"SGD\"),\n",
    "      loss = {\n",
    "          pheno_name:loss_\n",
    "      }, \n",
    "      metrics = [metrics_]#{'CobaltChloride_1':[metrics_],'CopperSulfate_1':[metrics_],'Diamide_1':[metrics_]}\n",
    "      )#change metrics to MSE ##['accuracy','mse']\n",
    "    return model\n",
    "\n",
    "def runModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode,pheno_index):\n",
    "    '''\n",
    "    def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "    pheno_index: the index of the phenotype in y_train\n",
    "    '''\n",
    "    history = model.fit(\n",
    "    x_train, \n",
    "    [y_train[:]],\n",
    "    validation_data=(x_valid, [y_valid[:]]),\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd899e2b-78b1-466f-bbe6-2e12e81ee0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel_all(pheno_name, pheno_index,hl_list, drop_list):\n",
    "    model_mse = model_build(pheno_name,4,hl_list,'relu','linear',drop_list,8,7,2,3)#model initiallization\n",
    "    model_mse = compile_model(pheno_name,model_mse,tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mse\"),\n",
    "    0.00123695244,'mse')\n",
    "    history_mse =  runModel(model_mse,0.30,32,400,15, \"val_mse\", \"min\",pheno_index)\n",
    "    return history_mse,model_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797144b-2685-4505-a310-baabdecccb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mse,model_mse = runModel_all('1_YPD_1',18,[1464,608,264,146],[0.00,0.00,0.00,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb8954-336c-42e4-9b0a-609b8815ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model_mse.predict(x_valid)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "print(\"Mean Squared Error on the validation set:\", mse)\n",
    "\n",
    "# Make predictions on the tesing set\n",
    "y_pred = model_mse.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on the testing set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf802e-a922-404c-9e6b-0c0d3c31411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Intersection of LIME and SHAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e70f2-5cb9-4d9f-a56b-667cff355db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_indexes(lime_indexes, shap_indexes):\n",
    "    # Convert lists to sets\n",
    "    lime_set = set(lime_indexes)\n",
    "    shap_set = set(shap_indexes)\n",
    "    \n",
    "    # Find the intersection\n",
    "    common_indexes = lime_set.intersection(shap_set)\n",
    "    \n",
    "    return common_indexes\n",
    "    \n",
    "\n",
    "common_indexes = find_common_indexes(indexes_lime, indexes_shap)\n",
    "print(type(common_indexes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd3acf-f7d2-46e2-a60c-b06730190e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indexes = np.array(list(common_indexes))\n",
    "(common_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbc19c-7fc0-4d80-8527-7a1371e5ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_indexes_ = common_indexes.tolist()\n",
    "len(common_indexes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e58bf-3a2c-4a7d-a5b8-b63a0f4fa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strings_at_indices(strings, indices):\n",
    "\n",
    "    valid_indices = [index for index in indices if 0 <= index < len(strings)]\n",
    "    \n",
    "    # Get strings at valid indices\n",
    "    result = [strings[index] for index in valid_indices]\n",
    "    \n",
    "    return result\n",
    "\n",
    "matching_features = get_strings_at_indices(feature_importance_df['Feature'].tolist(), common_indexes_)\n",
    "len(matching_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f98f36-62a5-4b81-b75b-37755b2df43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "genotypeFile = 'genotype.csv'\n",
    "genotype = pd.read_csv(genotypeFile, sep = '\\t', index_col = 0)\n",
    "print('genotypeFile shape:', genotype.shape )\n",
    "\n",
    "phenotypeFile = 'phenotype.csv'\n",
    "multi_pheno = pd.read_csv(phenotypeFile, sep = ',', index_col = 0)\n",
    "print('Phenotype_Multi shape:', multi_pheno.shape )\n",
    "\n",
    "\n",
    "# take a small part to test code\n",
    "# genotype\n",
    "X = genotype\n",
    "\n",
    "mask = ~X.columns.str.contains('|'.join(matching_features))\n",
    "\n",
    "# Set values to zero for non-matching columns\n",
    "X.loc[:, mask] = 0\n",
    "\n",
    "#display(X.head())\n",
    "# X = genotype.iloc[0:1000:, 0:5000]\n",
    "# single_pheno\n",
    "\n",
    "#Chage the index for each model here accoding to the correspinding model\n",
    "Y = multi_pheno.iloc[:, 18]#index=0 --> 1_YPD_1\n",
    "# # Y = multi_pheno.iloc[0:1000, pheno_i]\n",
    "\n",
    "\n",
    "# # Add noise\n",
    "# random missing masker\n",
    "missing_perc = 0.1\n",
    "nonmissing_ones = np.random.binomial(\n",
    "    1, 1 - missing_perc, size=X.shape[0] * X.shape[1])\n",
    "nonmissing_ones = nonmissing_ones.reshape(X.shape[0], X.shape[1])\n",
    "nonmissing_ones, nonmissing_ones.shape\n",
    "\n",
    "corrupted_X = X * nonmissing_ones\n",
    "# corrupted_X.head()\n",
    "\n",
    "# # Prepare data\n",
    "# ## One-hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_onehot = to_categorical(X)\n",
    "corrupted_X_onehot = to_categorical(corrupted_X)\n",
    "# corrupted_X_onehot.shape\n",
    "\n",
    "# normlization\n",
    "scaled_Y = (Y - Y.min()) / (Y.max() - Y.min())\n",
    "\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = 1.5 * IQR\n",
    "\n",
    "    outlier_indices = df[(df < Q1 - outlier_step) |\n",
    "                         (df > Q3 + outlier_step)].index\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "temp_Y = scaled_Y[~scaled_Y.isna()]\n",
    "outliers_index = detect_outliers(temp_Y)\n",
    "\n",
    "\n",
    "# set outliers as NAN\n",
    "scaled_Y_ = scaled_Y.copy()\n",
    "scaled_Y_[outliers_index] = np.nan\n",
    "\n",
    "\n",
    "# ## Split train and test\n",
    "train_X, test_X, corrupted_train_X, corrupted_test_X, train_Y, test_Y = train_test_split(\n",
    "    X, corrupted_X_onehot, scaled_Y_.iloc[:], test_size=0.1)\n",
    "\n",
    "# split df to train and valid\n",
    "train_X, valid_X, corrupted_train_X, corrupted_valid_X, train_Y, valid_Y = train_test_split(\n",
    "    train_X, corrupted_train_X, train_Y, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = to_categorical(pd.concat([train_X,train_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_train = train_Y.dropna().to_numpy()\n",
    "\n",
    "x_valid = to_categorical(pd.concat([valid_X,valid_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_valid = valid_Y.dropna().to_numpy()\n",
    "\n",
    "x_test = to_categorical(pd.concat([test_X,test_Y],axis=1).dropna().iloc[:,0:-1])\n",
    "y_test  = test_Y.dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06d31c-7d40-49f9-ac23-1d9e767e21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Retrain the model on the new dataset**\n",
    "\n",
    "def model_build(pheno_name,num_hl,hl_list,hl_activation, out_activation, dropout_val,filters_, kernel_size_,stride_poolSize,Conv_Layers_Stride_Size):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    https://towardsdatascience.com/multi-output-model-with-tensorflow-keras-functional-api-875dd89aa7c6\n",
    "\n",
    "    def DNN_build(num_hl,hl_list,hl_activation, out_activation, dropout_val):\n",
    "    pheno_name = name of phenotype of interest\n",
    "    num_hl = number of hidden layers\n",
    "    hl_list = list of hidden layers\n",
    "    hl_activation = hidden layer activation function\n",
    "    out_activation = output layer activation function\n",
    "    dropout_val = Dropout value\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    input_layer = tf.keras.Input(shape=(28220,3))#(3138, 28220, 3)\n",
    "    \n",
    "    shared_convL1 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation,input_shape = (28220,3))(input_layer)\n",
    "    shared_convL1_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL1)\n",
    "    \n",
    "    shared_convL2 = tf.keras.layers.Conv1D(filters = filters_,kernel_size = kernel_size_,strides = Conv_Layers_Stride_Size, activation = hl_activation)(shared_convL1_max_pool)\n",
    "    shared_convL2_max_pool = tf.keras.layers.MaxPool1D(pool_size=stride_poolSize, strides=stride_poolSize)(shared_convL2)\n",
    "    \n",
    "    shared_convLayer_Flatten = tf.keras.layers.Flatten()(shared_convL2_max_pool)\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    kernel_regularizer_ = tf.keras.regularizers.L1L2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    shared_hidden_layer1 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[0], activation  = hl_activation)(shared_convLayer_Flatten)\n",
    "    shared_hidden_layer1_dp1 = tf.keras.layers.Dropout(dropout_val[0])(shared_hidden_layer1)\n",
    "\n",
    "    #model 1\n",
    "    model1_hidden_layer2 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[1], activation  = hl_activation)(shared_hidden_layer1_dp1)\n",
    "    model1_hidden_layer2_dp2 = tf.keras.layers.Dropout(dropout_val[1])(model1_hidden_layer2)\n",
    "\n",
    "    model1_hidden_layer3 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[2], activation  = hl_activation)(model1_hidden_layer2_dp2)\n",
    "    model1_hidden_layer3_dp3 = tf.keras.layers.Dropout(dropout_val[2])(model1_hidden_layer3)\n",
    "\n",
    "    model1_hidden_layer4 = tf.keras.layers.Dense(kernel_regularizer=kernel_regularizer_,kernel_initializer=initializer,units = hl_list[3], activation  = hl_activation)(model1_hidden_layer3_dp3)\n",
    "    model1_hidden_layer4_dp4 = tf.keras.layers.Dropout(dropout_val[3])(model1_hidden_layer4)\n",
    "    model1_hidden_layer4_dp4_fl = tf.keras.layers.Flatten()(model1_hidden_layer4_dp4)\n",
    "    \n",
    "    pheno_name_ = tf.keras.layers.Dense(units = 1, name = pheno_name, activation  = out_activation)(model1_hidden_layer4_dp4_fl)#1_CobaltChloride_1\n",
    "    model = tf.keras.models.Model(inputs = input_layer, outputs = [pheno_name_])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35611b-5ee9-4e5f-9f8d-2175d5eebcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy','mse'])#change metrics to MSE\n",
    "def compile_model(pheno_name,model,loss_, learningRate, metrics_): #loss = 'sparse_categorical_crossentropy'\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest ['accuracy', 'mse']\n",
    "    '''\n",
    "\n",
    "    # Specify the optimizer, and compile the model with loss functions for both outputs\n",
    "    model.compile(\n",
    "       #optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate),#0.00027705 \n",
    "    optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learningRate, momentum=0.01, nesterov=True, name=\"SGD\"),\n",
    "      loss = {\n",
    "          pheno_name:loss_\n",
    "      }, \n",
    "      metrics = [metrics_]#{'CobaltChloride_1':[metrics_],'CopperSulfate_1':[metrics_],'Diamide_1':[metrics_]}\n",
    "      )#change metrics to MSE ##['accuracy','mse']\n",
    "    return model\n",
    "\n",
    "def runModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode,pheno_index):\n",
    "    '''\n",
    "    def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "    pheno_index: the index of the phenotype in y_train\n",
    "    '''\n",
    "    history = model.fit(\n",
    "    x_train, \n",
    "    [y_train[:]],\n",
    "    validation_data=(x_valid, [y_valid[:]]),\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f37917-6d1b-4636-87b4-58cef1980122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel_all(pheno_name, pheno_index,hl_list, drop_list):\n",
    "    model_mse = model_build(pheno_name,4,hl_list,'relu','linear',drop_list,8,7,2,3)#model initiallization\n",
    "    model_mse = compile_model(pheno_name,model_mse,tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mse\"),\n",
    "    0.00123695244,'mse')\n",
    "    history_mse =  runModel(model_mse,0.30,32,400,15, \"val_mse\", \"min\",pheno_index)\n",
    "    return history_mse,model_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca24c6-3e1e-4e33-b600-a38f043dc3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mse,model_mse = runModel_all('1_YPD_1',18,[1464,608,264,146],[0.00,0.00,0.00,0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887bf70-d75d-4d36-9b51-06052fbd56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model_mse.predict(x_valid)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "print(\"Mean Squared Error on the validation set:\", mse)\n",
    "\n",
    "# Make predictions on the tesing set\n",
    "y_pred = model_mse.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error on the testing set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52ebea-1af1-45b3-9525-db4182564c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977eea4-5728-4f80-a179-c2bffe394160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
